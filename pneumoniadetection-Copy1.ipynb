{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4bff5a0-fbdc-40e2-87a0-5b989a41a968",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ffdb143-4dc0-46c8-a2dd-656fa663b7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANTI-OVERFITTING Configuration\n",
    "TRAIN_DIR = \"data/train\"\n",
    "TEST_DIR = \"data/test\"\n",
    "IMG_SIZE = (224, 224)\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 30\n",
    "LEARNING_RATE = 0.0005  # REDUCED from 0.001\n",
    "SEED = 42\n",
    "PREDICTION_THRESHOLD = 0.65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5743fa98-f6bd-4a59-a5a4-5590d49f3332",
   "metadata": {},
   "outputs": [],
   "source": [
    " #Regularization\n",
    "DROPOUT_1 = 0.3  # Increased from 0.2\n",
    "DROPOUT_2 = 0.6  # Increased from 0.5\n",
    "L2_REG = 0.001   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2229a5a2-0b6c-4d5e-88a6-52a9b8d00326",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e6068c0-10bb-4020-bca5-7cc56c4601f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5232 files belonging to 2 classes.\n",
      "Found 624 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Load Data\n",
    "\n",
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    TRAIN_DIR, image_size=IMG_SIZE, batch_size=BATCH_SIZE,\n",
    "    label_mode=\"binary\", seed=SEED\n",
    ")\n",
    "test_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    TEST_DIR, image_size=IMG_SIZE, batch_size=BATCH_SIZE,\n",
    "    label_mode=\"binary\", shuffle=False, seed=SEED\n",
    ")\n",
    "class_names = train_ds.class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e37c544-8554-41ce-ba07-e8b223057155",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NORMAL', 'PNEUMONIA']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe30fdd6-4762-4795-b0be-73d2b2206047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REDUCED Augmentation (less aggressive)\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    tf.keras.layers.RandomFlip(\"horizontal\"),\n",
    "    tf.keras.layers.RandomRotation(0.1),  # REDUCED from 0.2\n",
    "    tf.keras.layers.RandomZoom(0.1),      # REDUCED from 0.2\n",
    "    # Removed RandomContrast\n",
    "], name=\"reduced_augmentation\")\n",
    "\n",
    "normalization = tf.keras.layers.Rescaling(1./127.5, offset=-1)\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "train_ds = train_ds.map(lambda x,y: (data_augmentation(x, training=True), y), num_parallel_calls=AUTOTUNE)\n",
    "train_ds = train_ds.map(lambda x,y: (normalization(x), y), num_parallel_calls=AUTOTUNE)\n",
    "test_ds = test_ds.map(lambda x,y: (normalization(x), y), num_parallel_calls=AUTOTUNE)\n",
    "train_ds = train_ds.prefetch(AUTOTUNE)\n",
    "test_ds = test_ds.prefetch(AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2985df43-ecfe-42af-b658-8a371e136bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Class weights: {0: 1.939214232765011, 1: 0.6737058975019315}\n"
     ]
    }
   ],
   "source": [
    "# Class Weights\n",
    "y_train = np.concatenate([y for x,y in train_ds], axis=0)\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train.flatten())\n",
    "class_weights_dict = dict(enumerate(class_weights))\n",
    "print(f\"✓ Class weights: {class_weights_dict}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06d25e19-ae02-469e-8d65-bb92054068c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building Anti-Overfitting Model...\n"
     ]
    }
   ],
   "source": [
    "# Build Anti-Overfitting Model\n",
    "print(\"\\nBuilding Anti-Overfitting Model...\")\n",
    "base_model = tf.keras.applications.MobileNetV2(\n",
    "    input_shape=(224,224,3), include_top=False, weights=\"imagenet\"\n",
    ")\n",
    "base_model.trainable = False\n",
    "\n",
    "l2_reg = tf.keras.regularizers.l2(L2_REG)\n",
    "\n",
    "inputs = tf.keras.Input(shape=(224,224,3))\n",
    "x = base_model(inputs, training=False)\n",
    "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a9e1effe-006b-40da-b3ea-47c93162baac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    }
   ],
   "source": [
    "# Anti-Overfitting Layers\n",
    "x = tf.keras.layers.BatchNormalization()(x)           # ADDED\n",
    "x = tf.keras.layers.Dropout(DROPOUT_1)(x)             # INCREASED\n",
    "x = tf.keras.layers.Dense(64, activation='relu',      # REDUCED from 128\n",
    "                          kernel_regularizer=l2_reg)(x)\n",
    "x = tf.keras.layers.BatchNormalization()(x)           # ADDED\n",
    "x = tf.keras.layers.Dropout(DROPOUT_2)(x)             # INCREASED\n",
    "\n",
    "outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "model = tf.keras.Model(inputs, outputs, name='AntiOverfitModel')\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(LEARNING_RATE),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy', tf.keras.metrics.AUC(name='auc')]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d929ffa3-1176-4568-ba9a-ea5e10409f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model built with:\n",
      "  + Batch Normalization (2 layers)\n",
      "  + Higher Dropout (0.3, 0.6)\n",
      "  + Smaller Dense Layer (64 vs 128)\n",
      "  + L2 Regularization (0.001)\n",
      "  + Lower Learning Rate (0.0005)\n"
     ]
    }
   ],
   "source": [
    "print(\"✓ Model built with:\")\n",
    "print(\"  + Batch Normalization (2 layers)\")\n",
    "print(f\"  + Higher Dropout ({DROPOUT_1}, {DROPOUT_2})\")\n",
    "print(\"  + Smaller Dense Layer (64 vs 128)\")\n",
    "print(f\"  + L2 Regularization ({L2_REG})\")\n",
    "print(f\"  + Lower Learning Rate ({LEARNING_RATE})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66811925-d53d-40a9-b636-aee376ac28e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggressive Callbacks\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss', patience=3, restore_best_weights=True, verbose=1\n",
    "    ),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss', factor=0.5, patience=2, min_lr=1e-7, verbose=1\n",
    "    ),\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        'best_model_antioverfit.keras', monitor='val_loss', \n",
    "        save_best_only=True, verbose=1\n",
    "    )\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8fba19eb-8ef9-4f4f-9630-c90fa52e6c33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.5238 - accuracy: 0.7942 - auc: 0.9253\n",
      "Epoch 1: val_loss improved from inf to 0.38023, saving model to best_model_antioverfit.keras\n",
      "164/164 [==============================] - 80s 461ms/step - loss: 0.5238 - accuracy: 0.7942 - auc: 0.9253 - val_loss: 0.3802 - val_accuracy: 0.8926 - val_auc: 0.9612 - lr: 5.0000e-04\n",
      "Epoch 2/30\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.3662 - accuracy: 0.8792 - auc: 0.9665\n",
      "Epoch 2: val_loss improved from 0.38023 to 0.33543, saving model to best_model_antioverfit.keras\n",
      "164/164 [==============================] - 58s 349ms/step - loss: 0.3662 - accuracy: 0.8792 - auc: 0.9665 - val_loss: 0.3354 - val_accuracy: 0.9006 - val_auc: 0.9687 - lr: 5.0000e-04\n",
      "Epoch 3/30\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.3246 - accuracy: 0.9077 - auc: 0.9743\n",
      "Epoch 3: val_loss did not improve from 0.33543\n",
      "164/164 [==============================] - 55s 330ms/step - loss: 0.3246 - accuracy: 0.9077 - auc: 0.9743 - val_loss: 0.3363 - val_accuracy: 0.9006 - val_auc: 0.9698 - lr: 5.0000e-04\n",
      "Epoch 4/30\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.2961 - accuracy: 0.9228 - auc: 0.9788\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.33543\n",
      "164/164 [==============================] - 64s 384ms/step - loss: 0.2961 - accuracy: 0.9228 - auc: 0.9788 - val_loss: 0.3709 - val_accuracy: 0.8766 - val_auc: 0.9632 - lr: 5.0000e-04\n",
      "Epoch 5/30\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.2844 - accuracy: 0.9266 - auc: 0.9807Restoring model weights from the end of the best epoch: 2.\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.33543\n",
      "164/164 [==============================] - 55s 329ms/step - loss: 0.2844 - accuracy: 0.9266 - auc: 0.9807 - val_loss: 0.3854 - val_accuracy: 0.8670 - val_auc: 0.9627 - lr: 2.5000e-04\n",
      "Epoch 5: early stopping\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "history = model.fit(\n",
    "    train_ds, validation_data=test_ds, epochs=EPOCHS,\n",
    "    class_weight=class_weights_dict, callbacks=callbacks, verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "52e33a61-763b-4ff3-b766-700d2224a745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "OVERFITTING ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "Final Results:\n",
      "  Train Loss: 0.2844 | Val Loss: 0.3854 | Gap: 0.1010\n",
      "  Train Acc:  0.9266 | Val Acc:  0.8670 | Gap: 0.0596\n",
      "\n",
      "Comparison with Original:\n",
      "  Original Gap: Loss 0.412, Acc 0.120 (12%)\n",
      "  New Gap:      Loss 0.101, Acc 0.060 (6.0%)\n",
      "\n",
      "✅ OVERFITTING SIGNIFICANTLY REDUCED!\n"
     ]
    }
   ],
   "source": [
    "# Analyze Overfitting\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"OVERFITTING ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "train_loss = history.history['loss'][-1]\n",
    "val_loss = history.history['val_loss'][-1]\n",
    "train_acc = history.history['accuracy'][-1]\n",
    "val_acc = history.history['val_accuracy'][-1]\n",
    "\n",
    "loss_gap = val_loss - train_loss\n",
    "acc_gap = train_acc - val_acc\n",
    "\n",
    "print(f\"\\nFinal Results:\")\n",
    "print(f\"  Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Gap: {loss_gap:.4f}\")\n",
    "print(f\"  Train Acc:  {train_acc:.4f} | Val Acc:  {val_acc:.4f} | Gap: {acc_gap:.4f}\")\n",
    "\n",
    "print(f\"\\nComparison with Original:\")\n",
    "print(f\"  Original Gap: Loss 0.412, Acc 0.120 (12%)\")\n",
    "print(f\"  New Gap:      Loss {loss_gap:.3f}, Acc {acc_gap:.3f} ({acc_gap*100:.1f}%)\")\n",
    "\n",
    "if loss_gap < 0.25 and acc_gap < 0.08:\n",
    "    print(\"\\n✅ OVERFITTING SIGNIFICANTLY REDUCED!\")\n",
    "elif loss_gap < 0.35:\n",
    "    print(\"\\n⚠️ OVERFITTING IMPROVED BUT STILL MODERATE\")\n",
    "else:\n",
    "    print(\"\\n❌ OVERFITTING STILL PRESENT\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "996a3b78-2c1b-4608-baf2-295ad8470968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "EVALUATION\n",
      "======================================================================\n",
      "Test Accuracy: 0.9006 (90.06%)\n",
      "\n",
      "Classification Report (Threshold 0.65):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      NORMAL       0.84      0.94      0.88       234\n",
      "   PNEUMONIA       0.96      0.89      0.92       390\n",
      "\n",
      "    accuracy                           0.91       624\n",
      "   macro avg       0.90      0.91      0.90       624\n",
      "weighted avg       0.91      0.91      0.91       624\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Evaluate\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EVALUATION\")\n",
    "print(\"=\"*70)\n",
    "results = model.evaluate(test_ds, verbose=0)\n",
    "print(f\"Test Accuracy: {results[1]:.4f} ({results[1]*100:.2f}%)\")\n",
    "\n",
    "# Predictions\n",
    "predictions = model.predict(test_ds, verbose=0)\n",
    "true_labels = np.concatenate([y for x,y in test_ds], axis=0).astype(int).flatten()\n",
    "pred_065 = (predictions > PREDICTION_THRESHOLD).astype(int).flatten()\n",
    "\n",
    "print(f\"\\nClassification Report (Threshold {PREDICTION_THRESHOLD}):\")\n",
    "print(classification_report(true_labels, pred_065, \n",
    "                          target_names=[str(n) for n in class_names]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "800add96-39f1-454c-94d7-31e1bd1eccf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Models saved\n",
      "\n",
      "======================================================================\n",
      "✅ TRAINING COMPLETE!\n",
      "======================================================================\n",
      "\n",
      "Files created:\n",
      "  - best_model_antioverfit.keras\n",
      "  - final_model_antioverfit.keras\n",
      "  - final_model_antioverfit.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "# Save\n",
    "model.save('final_model_antioverfit.keras')\n",
    "model.save('final_model_antioverfit.h5')\n",
    "print(\"\\n✓ Models saved\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"✅ TRAINING COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nFiles created:\")\n",
    "print(\"  - best_model_antioverfit.keras\")\n",
    "print(\"  - final_model_antioverfit.keras\")\n",
    "print(\"  - final_model_antioverfit.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d67aa3-d1d1-44d1-9ecc-e27173615577",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
